{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f485cae",
   "metadata": {},
   "source": [
    "# Data Transform\n",
    "\n",
    "In this notebook, we will ask you a series of questions to evaluate your findings from your EDA. Based on your response & justification, we will ask you to also apply a subsequent data transformation. \n",
    "\n",
    "If you state that you will not apply any data transformations for this step, you must **justify** as to why your dataset/machine-learning does not require the mentioned data preprocessing step.\n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient feature engineering step in this project we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "You will write out this transformed dataframe as a `.csv` file to your `data/` folder.\n",
    "\n",
    "**Note**: Again, note that this dataset is quite large. If you find that some data operations take too long to complete on your machine, simply use the `sample()` method to transform a subset of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a38922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3e2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/talgat/Downloads/detect-fraud/data/sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8cbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366e83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8275a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/talgat/Downloads/detect-fraud/data/transformed_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360ca62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50be5325",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Does your model contain any missing values or \"non-predictive\" columns? If so, which adjustments should you take to ensure that your model has good predictive capabilities? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "In this dataset, there aren’t any big missing values that need fixing. But some columns, like nameOrig and nameDest, are just ID numbers that don’t help the model find fraud. Also, the isFlaggedFraud column isn’t useful because it doesn’t match real fraud well and can confuse the model. So, it’s best to remove these columns so the model focuses only on helpful information and can better learn to detect fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2089cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/Users/talgat/Downloads/detect-fraud/data/sample.csv')\n",
    "\n",
    "# Drop columns that don't help the prediction\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud'])\n",
    "\n",
    "# Optionally, save the cleaned data for modeling\n",
    "df.to_csv('/Users/talgat/Downloads/detect-fraud/data/cleaned_transactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe64ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301be5ef",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Do certain transaction types consistently differ in amount or fraud likelihood? If so, how might you transform the type column to make this pattern usable by a machine learning model? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a46db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/talgat/Downloads/detect-fraud/data/sample.csv')\n",
    "\n",
    "# Drop non-predictive columns (if not already done)\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud'])\n",
    "\n",
    "# Apply one-hot encoding to the 'type' column\n",
    "df = pd.get_dummies(df, columns=['type'])\n",
    "\n",
    "# Save the transformed dataset for modeling\n",
    "df.to_csv('/Users/talgat/Downloads/detect-fraud/data/cleaned_transactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa820db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b952403f",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "After exploring your data, you may have noticed that fraudulent transactions are rare compared to non-fraudulent ones. What challenges might this pose when training a machine learning model? What strategies could you use to ensure your model learns meaningful patterns from the minority class? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b464622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9988    1.0000    0.9994      2496\n",
      "           1     1.0000    0.2500    0.4000         4\n",
      "\n",
      "    accuracy                         0.9988      2500\n",
      "   macro avg     0.9994    0.6250    0.6997      2500\n",
      "weighted avg     0.9988    0.9988    0.9984      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load cleaned and transformed data\n",
    "df = pd.read_csv('/Users/talgat/Downloads/detect-fraud/data/transformed_dataset.csv')\n",
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Use class_weight to help model focus on rare frauds\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f1422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17737e9e",
   "metadata": {},
   "source": [
    "## Bonus (optional)\n",
    "\n",
    "Are there interaction effects between variables (e.g., fraud and high amount and transaction type) that aren't captured directly in the dataset? Would it be helpful to manually engineer any new features that reflect these interactions? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cbfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out newly transformed dataset to your folder\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
